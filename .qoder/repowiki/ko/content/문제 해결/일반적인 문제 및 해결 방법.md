# 일반적인 문제 및 해결 방법

<cite>
**이 문서에서 참조한 파일**   
- [README.md](file://README.md)
- [environments/argocd/app-of-apps.yaml](file://environments/argocd/app-of-apps.yaml)
- [environments/argocd/apps/plate-api-prod.yaml](file://environments/argocd/apps/plate-api-prod.yaml)
- [environments/argocd/apps/plate-api-stg.yaml](file://environments/argocd/apps/plate-api-stg.yaml)
- [environments/argocd/apps/plate-web-stg.yaml](file://environments/argocd/apps/plate-web-stg.yaml)
- [environments/argocd/apps/plate-web-prod.yaml](file://environments/argocd/apps/plate-web-prod.yaml)
- [environments/argocd/apps/ingress-stg.yaml](file://environments/argocd/apps/ingress-stg.yaml)
- [helm/development-tools/argocd/values.yaml](file://helm/development-tools/argocd/values.yaml)
- [helm/cluster-services/cert-manager/values.yaml](file://helm/cluster-services/cert-manager/values.yaml)
- [helm/cluster-services/cert-manager/templates/cluster-issuer-prod.yaml](file://helm/cluster-services/cert-manager/templates/cluster-issuer-prod.yaml)
- [helm/cluster-services/cert-manager/templates/cluster-issuer-staging.yaml](file://helm/cluster-services/cert-manager/templates/cluster-issuer-staging.yaml)
- [helm/development-tools/grafana/values.yaml](file://helm/development-tools/grafana/values.yaml)
- [helm/development-tools/jenkins/values.yaml](file://helm/development-tools/jenkins/values.yaml)
- [helm/development-tools/prometheus/values.yaml](file://helm/development-tools/prometheus/values.yaml)
- [scripts/deploy-harbor-auth.sh](file://scripts/deploy-harbor-auth.sh)
- [scripts/verify-harbor-auth.sh](file://scripts/verify-harbor-auth.sh)
</cite>

## 목차
1. [ArgoCD 동기화 실패 문제 해결](#argocd-동기화-실패-문제-해결)
2. [Helm 차트 배포 오류](#helm-차트-배포-오류)
3. [인증서 갱신 실패 (Certificate Renewal Failure)](#인증서-갱신-실패-certificate-renewal-failure)
4. [Harbor 인증 문제](#harbor-인증-문제)
5. [Prometheus/Grafana 대시보드 연결 오류](#prometheusgrafana-대시보드-연결-오류)
6. [Jenkins 파이프라인 실패 시 초기화 절차](#jenkins-파이프라인-실패-시-초기화-절차)

## ArgoCD 동기화 실패 문제 해결

### 원인 분석
ArgoCD 동기화 실패는 주로 다음과 같은 원인으로 발생합니다:
- Git 저장소와 클러스터 상태 간의 불일치
- Helm 차트의 `values.yaml` 파일에 문법 오류 또는 유효하지 않은 값 포함
- 네임스페이스 또는 리소스 권한 부족
- ArgoCD Application 정의 파일의 경로 오류
- 네트워크 문제 또는 API 서버 접근 실패

특히, `environments/argocd/apps/` 디렉터리에 위치한 각 애플리케이션의 ArgoCD Application 정의 파일(예: `plate-api-prod.yaml`)에서 `source.path` 또는 `destination.namespace` 값이 잘못 설정된 경우 동기화가 실패할 수 있습니다.

### 로그 확인 위치
- ArgoCD UI에서 동기화 실패한 애플리케이션의 **Logs** 탭 확인
- ArgoCD 컨트롤러 Pod 로그 확인:
  ```bash
  kubectl logs -n argocd -l app.kubernetes.io/name=argocd-application-controller
  ```

### 진단 명령어
- 동기화 상태 확인:
  ```bash
  argocd app get plate-api-prod
  ```
- 모든 애플리케이션 상태 확인:
  ```bash
  argocd app list
  ```
- 동기화 시도:
  ```bash
  argocd app sync plate-api-prod
  ```

### 해결 절차
1. ArgoCD UI 또는 CLI를 통해 동기화 실패 원인 확인
2. 관련 Helm 차트의 `values.yaml` 파일 문법 검증:
   ```bash
   helm lint helm/applications/plate-server
   ```
3. Git 저장소의 변경 사항이 정상적으로 커밋되었는지 확인
4. `environments/argocd/apps/` 디렉터리의 Application 정의 파일에서 `source.path`, `destination.namespace` 값 확인
5. 수동으로 동기화 재시도:
   ```bash
   argocd app sync plate-api-prod --force
   ```
6. 지속적인 실패 시, `syncPolicy.retry` 설정을 통해 자동 재시도 기능 활용

**Section sources**
- [environments/argocd/apps/plate-api-prod.yaml](file://environments/argocd/apps/plate-api-prod.yaml)
- [environments/argocd/apps/plate-api-stg.yaml](file://environments/argocd/apps/plate-api-stg.yaml)
- [environments/argocd/apps/plate-web-stg.yaml](file://environments/argocd/apps/plate-web-stg.yaml)
- [environments/argocd/apps/plate-web-prod.yaml](file://environments/argocd/apps/plate-web-prod.yaml)

## Helm 차트 배포 오류

### 원인 분석
Helm 차트 배포 오류는 다음 요인으로 발생할 수 있습니다:
- `values.yaml` 파일의 키-값 구조 오류
- 의존성 차트(Dependency) 누락 또는 버전 불일치
- Pod 보안 정책(PSP) 또는 네임스페이스 제약 조건 위반
- 리소스 요청/제한(Requests/Limits) 설정 오류

### 로그 확인 위치
- Helm 설치 시 출력되는 오류 메시지
- 관련 Pod의 로그:
  ```bash
  kubectl logs -n <namespace> <pod-name>
  ```

### 진단 명령어
- 차트 렌더링 확인:
  ```bash
  helm template helm/applications/plate-server -f helm/applications/plate-server/values-prod.yaml
  ```
- 설치 전 검증:
  ```bash
  helm install --dry-run --debug my-release helm/applications/plate-server
  ```

### 해결 절차
1. `helm lint` 명령어로 차트 구조 검증
2. `helm template`으로 YAML 출력 확인 및 문법 오류 검사
3. `--dry-run` 옵션으로 설치 시뮬레이션 수행
4. 실제 환경과 일치하는 `values.yaml` 파일 사용 여부 확인
5. 필요한 경우 `--set` 옵션으로 동적 값 주입 후 재시도

## 인증서 갱신 실패 (Certificate Renewal Failure)

### 원인 분석
cert-manager를 통한 인증서 갱신 실패는 주로 다음 이유로 발생합니다:
- `ClusterIssuer` 설정 오류
- DNS 또는 Ingress 설정 문제로 인한 ACME 도전(Challenge) 실패
- Let's Encrypt rate limit 초과
- Secret 리소스 접근 권한 문제

`helm/cluster-services/cert-manager/values.yaml` 파일에서 `clusterIssuer.production` 및 `clusterIssuer.staging` 설정이 올바르게 구성되어야 하며, `cert-manager.io/cluster-issuer` 어노테이션이 Ingress 리소스에 정확히 지정되어야 합니다.

### 로그 확인 위치
- cert-manager 컨트롤러 로그:
  ```bash
  kubectl logs -n cert-manager -l app.kubernetes.io/name=cert-manager
  ```
- Certificate, Order, Challenge 리소스 상태 확인:
  ```bash
  kubectl get certificates -A
  kubectl get orders -A
  kubectl get challenges -A
  ```

### 진단 명령어
- 인증서 상태 확인:
  ```bash
  kubectl describe certificate -n <namespace> <certificate-name>
  ```
- Order 및 Challenge 상태 확인:
  ```bash
  kubectl describe order -n <namespace> <order-name>
  kubectl describe challenge -n <namespace> <challenge-name>
  ```

### 해결 절차
1. `helm/cluster-services/cert-manager/values.yaml`에서 `clusterIssuer` 설정 확인
2. Ingress 리소스의 `cert-manager.io/cluster-issuer` 어노테이션 확인
3. `kubectl get certificates -A`로 인증서 상태 확인
4. `describe` 명령어로 실패 원인 분석
5. DNS A/CNAME 레코드가 Ingress Controller의 LoadBalancer IP와 매칭되는지 확인
6. 필요 시 기존 인증서 삭제 후 재생성:
  ```bash
  kubectl delete certificate <cert-name> -n <namespace>
  ```

**Section sources**
- [helm/cluster-services/cert-manager/values.yaml](file://helm/cluster-services/cert-manager/values.yaml)
- [helm/cluster-services/cert-manager/templates/cluster-issuer-prod.yaml](file://helm/cluster-services/cert-manager/templates/cluster-issuer-prod.yaml)
- [helm/cluster-services/cert-manager/templates/cluster-issuer-staging.yaml](file://helm/cluster-services/cert-manager/templates/cluster-issuer-staging.yaml)

## Harbor 인증 문제

### 원인 분석
Harbor 인증 문제는 다음과 같은 상황에서 발생합니다:
- OpenBao에 저장된 Harbor 인증 정보가 유효하지 않음
- External Secrets Operator(ESO)가 OpenBao에서 비밀을 가져오지 못함
- `harbor-docker-secret`이 생성되지 않음
- Pod의 `imagePullSecrets` 설정 누락

`scripts/deploy-harbor-auth.sh` 스크립트는 ArgoCD를 통해 Harbor 인증 ESO 리소스를 배포하며, `scripts/verify-harbor-auth.sh` 스크립트는 인증 설정을 검증합니다.

### 로그 확인 위치
- ESO Pod 로그:
  ```bash
  kubectl logs -n external-secrets-system -l app.kubernetes.io/name=external-secrets
  ```
- SecretStore 및 ExternalSecret 상태 확인:
  ```bash
  kubectl get secretstore -A
  kubectl get externalsecret -A
  ```

### 진단 명령어
- ESO 상태 확인:
  ```bash
  kubectl get pods -n external-secrets-system
  ```
- Secret 생성 확인:
  ```bash
  kubectl get secret harbor-docker-secret -n plate-stg
  ```
- 인증 검증 스크립트 실행:
  ```bash
  ./scripts/verify-harbor-auth.sh
  ```

### 해결 절차
1. `scripts/deploy-harbor-auth.sh` 스크립트 실행하여 ESO 리소스 배포
2. `scripts/verify-harbor-auth.sh` 스크립트로 인증 상태 검증
3. OpenBao 토큰이 `openbao-token-secret.yaml`에 올바르게 설정되었는지 확인
4. `harbor-auth.yaml` ArgoCD Application이 정상적으로 동기화되었는지 확인
5. `harbor-docker-secret`이 각 네임스페이스에 생성되었는지 확인
6. 테스트 Pod을 생성하여 Harbor 이미지 Pull 테스트 수행

**Section sources**
- [scripts/deploy-harbor-auth.sh](file://scripts/deploy-harbor-auth.sh)
- [scripts/verify-harbor-auth.sh](file://scripts/verify-harbor-auth.sh)

## Prometheus/Grafana 대시보드 연결 오류

### 원인 분석
Prometheus/Grafana 대시보드 연결 오류는 다음 원인으로 발생할 수 있습니다:
- Ingress 설정 오류로 인한 외부 접근 실패
- TLS 인증서 문제
- 데이터 소스(Data Source) 설정 미완료
- 대시보드 임포트 실패

`helm/development-tools/grafana/values.yaml` 파일에서 `datasources` 설정이 Prometheus 서버를 올바르게 가리키고 있어야 하며, `ingress` 설정에서 `cert-manager.io/cluster-issuer` 어노테이션이 정확히 지정되어야 합니다.

### 로그 확인 위치
- Grafana Pod 로그:
  ```bash
  kubectl logs -n monitoring -l app.kubernetes.io/name=grafana
  ```
- Prometheus Pod 로그:
  ```bash
  kubectl logs -n monitoring -l app.kubernetes.io/name=prometheus-server
  ```

### 진단 명령어
- 서비스 상태 확인:
  ```bash
  kubectl get svc -n monitoring
  ```
- Ingress 상태 확인:
  ```bash
  kubectl get ingress -n monitoring
  ```
- 대시보드 연결 테스트:
  ```bash
  curl -k https://grafana.cocdev.co.kr
  ```

### 해결 절차
1. `helm/development-tools/grafana/values.yaml`에서 `datasources` 설정 확인
2. Ingress 리소스의 `host` 및 `tls` 설정 확인
3. `kubectl get certificates -n monitoring`으로 인증서 상태 확인
4. Grafana UI에서 데이터 소스 연결 테스트
5. 대시보드 JSON 파일이 올바르게 임포트되었는지 확인
6. 필요 시 Ingress 재생성 또는 인증서 갱신

**Section sources**
- [helm/development-tools/grafana/values.yaml](file://helm/development-tools/grafana/values.yaml)
- [helm/development-tools/prometheus/values.yaml](file://helm/development-tools/prometheus/values.yaml)

## Jenkins 파이프라인 실패 시 초기화 절차

### 원인 분석
Jenkins 파이프라인 실패는 다음과 같은 이유로 발생할 수 있습니다:
- 플러그인 충돌 또는 버전 호환성 문제
- JCasC(Configuration as Code) 설정 오류
- Agent 연결 실패
- 스크립트 보안 정책 위반

`helm/development-tools/jenkins/values.yaml` 파일에서 `controller.installPlugins`, `controller.JCasC.configScripts` 설정이 올바르게 구성되어야 하며, `controller.sidecars.configAutoReload.enabled` 설정을 통해 설정 변경 시 자동 재시작을 방지할 수 있습니다.

### 로그 확인 위치
- Jenkins 컨트롤러 Pod 로그:
  ```bash
  kubectl logs -n ci-cd -l app.kubernetes.io/component=jenkins-controller
  ```
- Jenkins UI의 파이프라인 콘솔 출력

### 진단 명령어
- Pod 상태 확인:
  ```bash
  kubectl get pods -n ci-cd
  ```
- 리소스 사용량 확인:
  ```bash
  kubectl top pods -n ci-cd
  ```

### 해결 절차
1. `helm/development-tools/jenkins/values.yaml`에서 `controller.installPlugins` 및 `JCasC` 설정 확인
2. `controller.initializeOnce: false`로 설정하여 플러그인 업데이트 허용
3. `controller.overwritePlugins: true`로 설정하여 플러그인 강제 재설치
4. Jenkins Pod 재시작:
  ```bash
  kubectl delete pod -n ci-cd <jenkins-pod-name>
  ```
5. Jenkins UI에서 파이프라인 재시작 또는 설정 재적용

**Section sources**
- [helm/development-tools/jenkins/values.yaml](file://helm/development-tools/jenkins/values.yaml)